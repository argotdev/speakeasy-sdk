/*
 * Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.
 */

import * as z from "zod";
import { remap as remap$ } from "../../lib/primitives.js";
import { ClosedEnum } from "../../types/enums.js";
import {
  ChatCompletionFunctionCallOption,
  ChatCompletionFunctionCallOption$inboundSchema,
  ChatCompletionFunctionCallOption$Outbound,
  ChatCompletionFunctionCallOption$outboundSchema,
} from "./chatcompletionfunctioncalloption.js";
import {
  ChatCompletionFunctions,
  ChatCompletionFunctions$inboundSchema,
  ChatCompletionFunctions$Outbound,
  ChatCompletionFunctions$outboundSchema,
} from "./chatcompletionfunctions.js";
import {
  ChatCompletionRequestMessage,
  ChatCompletionRequestMessage$inboundSchema,
  ChatCompletionRequestMessage$Outbound,
  ChatCompletionRequestMessage$outboundSchema,
} from "./chatcompletionrequestmessage.js";
import {
  ChatCompletionStreamOptions,
  ChatCompletionStreamOptions$inboundSchema,
  ChatCompletionStreamOptions$Outbound,
  ChatCompletionStreamOptions$outboundSchema,
} from "./chatcompletionstreamoptions.js";
import {
  ChatCompletionTool,
  ChatCompletionTool$inboundSchema,
  ChatCompletionTool$Outbound,
  ChatCompletionTool$outboundSchema,
} from "./chatcompletiontool.js";
import {
  ChatCompletionToolChoiceOption,
  ChatCompletionToolChoiceOption$inboundSchema,
  ChatCompletionToolChoiceOption$Outbound,
  ChatCompletionToolChoiceOption$outboundSchema,
} from "./chatcompletiontoolchoiceoption.js";
import {
  ResponseFormatJsonObject,
  ResponseFormatJsonObject$inboundSchema,
  ResponseFormatJsonObject$Outbound,
  ResponseFormatJsonObject$outboundSchema,
} from "./responseformatjsonobject.js";
import {
  ResponseFormatJsonSchema,
  ResponseFormatJsonSchema$inboundSchema,
  ResponseFormatJsonSchema$Outbound,
  ResponseFormatJsonSchema$outboundSchema,
} from "./responseformatjsonschema.js";
import {
  ResponseFormatText,
  ResponseFormatText$inboundSchema,
  ResponseFormatText$Outbound,
  ResponseFormatText$outboundSchema,
} from "./responseformattext.js";

export const Two = {
  O1Preview: "o1-preview",
  O1Preview20240912: "o1-preview-2024-09-12",
  O1Mini: "o1-mini",
  O1Mini20240912: "o1-mini-2024-09-12",
  Gpt4o: "gpt-4o",
  Gpt4o20240806: "gpt-4o-2024-08-06",
  Gpt4o20240513: "gpt-4o-2024-05-13",
  Chatgpt4oLatest: "chatgpt-4o-latest",
  Gpt4oMini: "gpt-4o-mini",
  Gpt4oMini20240718: "gpt-4o-mini-2024-07-18",
  Gpt4Turbo: "gpt-4-turbo",
  Gpt4Turbo20240409: "gpt-4-turbo-2024-04-09",
  Gpt40125Preview: "gpt-4-0125-preview",
  Gpt4TurboPreview: "gpt-4-turbo-preview",
  Gpt41106Preview: "gpt-4-1106-preview",
  Gpt4VisionPreview: "gpt-4-vision-preview",
  Gpt4: "gpt-4",
  Gpt40314: "gpt-4-0314",
  Gpt40613: "gpt-4-0613",
  Gpt432k: "gpt-4-32k",
  Gpt432k0314: "gpt-4-32k-0314",
  Gpt432k0613: "gpt-4-32k-0613",
  Gpt35Turbo: "gpt-3.5-turbo",
  Gpt35Turbo16k: "gpt-3.5-turbo-16k",
  Gpt35Turbo0301: "gpt-3.5-turbo-0301",
  Gpt35Turbo0613: "gpt-3.5-turbo-0613",
  Gpt35Turbo1106: "gpt-3.5-turbo-1106",
  Gpt35Turbo0125: "gpt-3.5-turbo-0125",
  Gpt35Turbo16k0613: "gpt-3.5-turbo-16k-0613",
} as const;
export type Two = ClosedEnum<typeof Two>;

/**
 * ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API.
 */
export type CreateChatCompletionRequestModel = string | Two;

/**
 * An object specifying the format that the model must output. Compatible with [GPT-4o](/docs/models/gpt-4o), [GPT-4o mini](/docs/models/gpt-4o-mini), [GPT-4 Turbo](/docs/models/gpt-4-and-gpt-4-turbo) and all GPT-3.5 Turbo models newer than `gpt-3.5-turbo-1106`.
 *
 * @remarks
 *
 * Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema. Learn more in the [Structured Outputs guide](/docs/guides/structured-outputs).
 *
 * Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the message the model generates is valid JSON.
 *
 * **Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if `finish_reason="length"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.
 */
export type ResponseFormat =
  | ResponseFormatText
  | ResponseFormatJsonObject
  | ResponseFormatJsonSchema;

/**
 * Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:
 *
 * @remarks
 *   - If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.
 *   - If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.
 *   - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.
 *   - When not set, the default behavior is 'auto'.
 *
 *   When this parameter is set, the response body will include the `service_tier` utilized.
 */
export const ServiceTier = {
  Auto: "auto",
  Default: "default",
} as const;
/**
 * Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:
 *
 * @remarks
 *   - If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.
 *   - If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.
 *   - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.
 *   - When not set, the default behavior is 'auto'.
 *
 *   When this parameter is set, the response body will include the `service_tier` utilized.
 */
export type ServiceTier = ClosedEnum<typeof ServiceTier>;

/**
 * Up to 4 sequences where the API will stop generating further tokens.
 *
 * @remarks
 */
export type Stop = string | Array<string>;

/**
 * `none` means the model will not call a function and instead generates a message. `auto` means the model can pick between generating a message or calling a function.
 *
 * @remarks
 */
export const One = {
  None: "none",
  Auto: "auto",
} as const;
/**
 * `none` means the model will not call a function and instead generates a message. `auto` means the model can pick between generating a message or calling a function.
 *
 * @remarks
 */
export type One = ClosedEnum<typeof One>;

/**
 * Deprecated in favor of `tool_choice`.
 *
 * @remarks
 *
 * Controls which (if any) function is called by the model.
 * `none` means the model will not call a function and instead generates a message.
 * `auto` means the model can pick between generating a message or calling a function.
 * Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.
 *
 * `none` is the default when no functions are present. `auto` is the default if functions are present.
 *
 * @deprecated class: This will be removed in a future release, please migrate away from it as soon as possible.
 */
export type CreateChatCompletionRequestFunctionCall =
  | ChatCompletionFunctionCallOption
  | One;

export type CreateChatCompletionRequest = {
  /**
   * A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).
   */
  messages: Array<ChatCompletionRequestMessage>;
  /**
   * ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API.
   */
  model: string | Two;
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
   *
   * @remarks
   *
   * [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)
   */
  frequencyPenalty?: number | null | undefined;
  /**
   * Modify the likelihood of specified tokens appearing in the completion.
   *
   * @remarks
   *
   * Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
   */
  logitBias?: { [k: string]: number } | null | undefined;
  /**
   * Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.
   */
  logprobs?: boolean | null | undefined;
  /**
   * An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.
   */
  topLogprobs?: number | null | undefined;
  /**
   * The maximum number of [tokens](/tokenizer) that can be generated in the chat completion. This value can be used to control [costs](https://openai.com/api/pricing/) for text generated via API.
   *
   * @remarks
   *
   * This value is now deprecated in favor of `max_completion_tokens`, and is not compatible with [o1 series models](/docs/guides/reasoning).
   *
   * @deprecated field: This will be removed in a future release, please migrate away from it as soon as possible.
   */
  maxTokens?: number | null | undefined;
  /**
   * An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and [reasoning tokens](/docs/guides/reasoning).
   *
   * @remarks
   */
  maxCompletionTokens?: number | null | undefined;
  /**
   * How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
   */
  n?: number | null | undefined;
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
   *
   * @remarks
   *
   * [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)
   */
  presencePenalty?: number | null | undefined;
  /**
   * An object specifying the format that the model must output. Compatible with [GPT-4o](/docs/models/gpt-4o), [GPT-4o mini](/docs/models/gpt-4o-mini), [GPT-4 Turbo](/docs/models/gpt-4-and-gpt-4-turbo) and all GPT-3.5 Turbo models newer than `gpt-3.5-turbo-1106`.
   *
   * @remarks
   *
   * Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema. Learn more in the [Structured Outputs guide](/docs/guides/structured-outputs).
   *
   * Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the message the model generates is valid JSON.
   *
   * **Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if `finish_reason="length"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.
   */
  responseFormat?:
    | ResponseFormatText
    | ResponseFormatJsonObject
    | ResponseFormatJsonSchema
    | undefined;
  /**
   * This feature is in Beta.
   *
   * @remarks
   * If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.
   * Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
   */
  seed?: number | null | undefined;
  /**
   * Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:
   *
   * @remarks
   *   - If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.
   *   - If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.
   *   - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.
   *   - When not set, the default behavior is 'auto'.
   *
   *   When this parameter is set, the response body will include the `service_tier` utilized.
   */
  serviceTier?: ServiceTier | null | undefined;
  /**
   * Up to 4 sequences where the API will stop generating further tokens.
   *
   * @remarks
   */
  stop?: string | Array<string> | null | undefined;
  /**
   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).
   *
   * @remarks
   */
  stream?: boolean | null | undefined;
  /**
   * Options for streaming response. Only set this when you set `stream: true`.
   *
   * @remarks
   */
  streamOptions?: ChatCompletionStreamOptions | null | undefined;
  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
   *
   * @remarks
   *
   * We generally recommend altering this or `top_p` but not both.
   */
  temperature?: number | null | undefined;
  /**
   * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
   *
   * @remarks
   *
   * We generally recommend altering this or `temperature` but not both.
   */
  topP?: number | null | undefined;
  /**
   * A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.
   *
   * @remarks
   */
  tools?: Array<ChatCompletionTool> | undefined;
  /**
   * Controls which (if any) tool is called by the model.
   *
   * @remarks
   * `none` means the model will not call any tool and instead generates a message.
   * `auto` means the model can pick between generating a message or calling one or more tools.
   * `required` means the model must call one or more tools.
   * Specifying a particular tool via `{"type": "function", "function": {"name": "my_function"}}` forces the model to call that tool.
   *
   * `none` is the default when no tools are present. `auto` is the default if tools are present.
   */
  toolChoice?: ChatCompletionToolChoiceOption | undefined;
  /**
   * Whether to enable [parallel function calling](/docs/guides/function-calling/parallel-function-calling) during tool use.
   */
  parallelToolCalls?: boolean | undefined;
  /**
   * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
   *
   * @remarks
   */
  user?: string | undefined;
  /**
   * Deprecated in favor of `tool_choice`.
   *
   * @remarks
   *
   * Controls which (if any) function is called by the model.
   * `none` means the model will not call a function and instead generates a message.
   * `auto` means the model can pick between generating a message or calling a function.
   * Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.
   *
   * `none` is the default when no functions are present. `auto` is the default if functions are present.
   *
   * @deprecated field: This will be removed in a future release, please migrate away from it as soon as possible.
   */
  functionCall?: ChatCompletionFunctionCallOption | One | undefined;
  /**
   * Deprecated in favor of `tools`.
   *
   * @remarks
   *
   * A list of functions the model may generate JSON inputs for.
   *
   * @deprecated field: This will be removed in a future release, please migrate away from it as soon as possible.
   */
  functions?: Array<ChatCompletionFunctions> | undefined;
};

/** @internal */
export const Two$inboundSchema: z.ZodNativeEnum<typeof Two> = z.nativeEnum(Two);

/** @internal */
export const Two$outboundSchema: z.ZodNativeEnum<typeof Two> =
  Two$inboundSchema;

/**
 * @internal
 * @deprecated This namespace will be removed in future versions. Use schemas and types that are exported directly from this module.
 */
export namespace Two$ {
  /** @deprecated use `Two$inboundSchema` instead. */
  export const inboundSchema = Two$inboundSchema;
  /** @deprecated use `Two$outboundSchema` instead. */
  export const outboundSchema = Two$outboundSchema;
}

/** @internal */
export const CreateChatCompletionRequestModel$inboundSchema: z.ZodType<
  CreateChatCompletionRequestModel,
  z.ZodTypeDef,
  unknown
> = z.union([z.string(), Two$inboundSchema]);

/** @internal */
export type CreateChatCompletionRequestModel$Outbound = string | string;

/** @internal */
export const CreateChatCompletionRequestModel$outboundSchema: z.ZodType<
  CreateChatCompletionRequestModel$Outbound,
  z.ZodTypeDef,
  CreateChatCompletionRequestModel
> = z.union([z.string(), Two$outboundSchema]);

/**
 * @internal
 * @deprecated This namespace will be removed in future versions. Use schemas and types that are exported directly from this module.
 */
export namespace CreateChatCompletionRequestModel$ {
  /** @deprecated use `CreateChatCompletionRequestModel$inboundSchema` instead. */
  export const inboundSchema = CreateChatCompletionRequestModel$inboundSchema;
  /** @deprecated use `CreateChatCompletionRequestModel$outboundSchema` instead. */
  export const outboundSchema = CreateChatCompletionRequestModel$outboundSchema;
  /** @deprecated use `CreateChatCompletionRequestModel$Outbound` instead. */
  export type Outbound = CreateChatCompletionRequestModel$Outbound;
}

/** @internal */
export const ResponseFormat$inboundSchema: z.ZodType<
  ResponseFormat,
  z.ZodTypeDef,
  unknown
> = z.union([
  ResponseFormatText$inboundSchema,
  ResponseFormatJsonObject$inboundSchema,
  ResponseFormatJsonSchema$inboundSchema,
]);

/** @internal */
export type ResponseFormat$Outbound =
  | ResponseFormatText$Outbound
  | ResponseFormatJsonObject$Outbound
  | ResponseFormatJsonSchema$Outbound;

/** @internal */
export const ResponseFormat$outboundSchema: z.ZodType<
  ResponseFormat$Outbound,
  z.ZodTypeDef,
  ResponseFormat
> = z.union([
  ResponseFormatText$outboundSchema,
  ResponseFormatJsonObject$outboundSchema,
  ResponseFormatJsonSchema$outboundSchema,
]);

/**
 * @internal
 * @deprecated This namespace will be removed in future versions. Use schemas and types that are exported directly from this module.
 */
export namespace ResponseFormat$ {
  /** @deprecated use `ResponseFormat$inboundSchema` instead. */
  export const inboundSchema = ResponseFormat$inboundSchema;
  /** @deprecated use `ResponseFormat$outboundSchema` instead. */
  export const outboundSchema = ResponseFormat$outboundSchema;
  /** @deprecated use `ResponseFormat$Outbound` instead. */
  export type Outbound = ResponseFormat$Outbound;
}

/** @internal */
export const ServiceTier$inboundSchema: z.ZodNativeEnum<typeof ServiceTier> = z
  .nativeEnum(ServiceTier);

/** @internal */
export const ServiceTier$outboundSchema: z.ZodNativeEnum<typeof ServiceTier> =
  ServiceTier$inboundSchema;

/**
 * @internal
 * @deprecated This namespace will be removed in future versions. Use schemas and types that are exported directly from this module.
 */
export namespace ServiceTier$ {
  /** @deprecated use `ServiceTier$inboundSchema` instead. */
  export const inboundSchema = ServiceTier$inboundSchema;
  /** @deprecated use `ServiceTier$outboundSchema` instead. */
  export const outboundSchema = ServiceTier$outboundSchema;
}

/** @internal */
export const Stop$inboundSchema: z.ZodType<Stop, z.ZodTypeDef, unknown> = z
  .union([z.string(), z.array(z.string())]);

/** @internal */
export type Stop$Outbound = string | Array<string>;

/** @internal */
export const Stop$outboundSchema: z.ZodType<Stop$Outbound, z.ZodTypeDef, Stop> =
  z.union([z.string(), z.array(z.string())]);

/**
 * @internal
 * @deprecated This namespace will be removed in future versions. Use schemas and types that are exported directly from this module.
 */
export namespace Stop$ {
  /** @deprecated use `Stop$inboundSchema` instead. */
  export const inboundSchema = Stop$inboundSchema;
  /** @deprecated use `Stop$outboundSchema` instead. */
  export const outboundSchema = Stop$outboundSchema;
  /** @deprecated use `Stop$Outbound` instead. */
  export type Outbound = Stop$Outbound;
}

/** @internal */
export const One$inboundSchema: z.ZodNativeEnum<typeof One> = z.nativeEnum(One);

/** @internal */
export const One$outboundSchema: z.ZodNativeEnum<typeof One> =
  One$inboundSchema;

/**
 * @internal
 * @deprecated This namespace will be removed in future versions. Use schemas and types that are exported directly from this module.
 */
export namespace One$ {
  /** @deprecated use `One$inboundSchema` instead. */
  export const inboundSchema = One$inboundSchema;
  /** @deprecated use `One$outboundSchema` instead. */
  export const outboundSchema = One$outboundSchema;
}

/** @internal */
export const CreateChatCompletionRequestFunctionCall$inboundSchema: z.ZodType<
  CreateChatCompletionRequestFunctionCall,
  z.ZodTypeDef,
  unknown
> = z.union([
  ChatCompletionFunctionCallOption$inboundSchema,
  One$inboundSchema,
]);

/** @internal */
export type CreateChatCompletionRequestFunctionCall$Outbound =
  | ChatCompletionFunctionCallOption$Outbound
  | string;

/** @internal */
export const CreateChatCompletionRequestFunctionCall$outboundSchema: z.ZodType<
  CreateChatCompletionRequestFunctionCall$Outbound,
  z.ZodTypeDef,
  CreateChatCompletionRequestFunctionCall
> = z.union([
  ChatCompletionFunctionCallOption$outboundSchema,
  One$outboundSchema,
]);

/**
 * @internal
 * @deprecated This namespace will be removed in future versions. Use schemas and types that are exported directly from this module.
 */
export namespace CreateChatCompletionRequestFunctionCall$ {
  /** @deprecated use `CreateChatCompletionRequestFunctionCall$inboundSchema` instead. */
  export const inboundSchema =
    CreateChatCompletionRequestFunctionCall$inboundSchema;
  /** @deprecated use `CreateChatCompletionRequestFunctionCall$outboundSchema` instead. */
  export const outboundSchema =
    CreateChatCompletionRequestFunctionCall$outboundSchema;
  /** @deprecated use `CreateChatCompletionRequestFunctionCall$Outbound` instead. */
  export type Outbound = CreateChatCompletionRequestFunctionCall$Outbound;
}

/** @internal */
export const CreateChatCompletionRequest$inboundSchema: z.ZodType<
  CreateChatCompletionRequest,
  z.ZodTypeDef,
  unknown
> = z.object({
  messages: z.array(ChatCompletionRequestMessage$inboundSchema),
  model: z.union([z.string(), Two$inboundSchema]),
  frequency_penalty: z.nullable(z.number().default(0)),
  logit_bias: z.nullable(z.record(z.number().int())).optional(),
  logprobs: z.nullable(z.boolean().default(false)),
  top_logprobs: z.nullable(z.number().int()).optional(),
  max_tokens: z.nullable(z.number().int()).optional(),
  max_completion_tokens: z.nullable(z.number().int()).optional(),
  n: z.nullable(z.number().int().default(1)),
  presence_penalty: z.nullable(z.number().default(0)),
  response_format: z.union([
    ResponseFormatText$inboundSchema,
    ResponseFormatJsonObject$inboundSchema,
    ResponseFormatJsonSchema$inboundSchema,
  ]).optional(),
  seed: z.nullable(z.number().int()).optional(),
  service_tier: z.nullable(ServiceTier$inboundSchema).default(null),
  stop: z.nullable(z.union([z.string(), z.array(z.string())])).optional(),
  stream: z.nullable(z.boolean().default(false)),
  stream_options: z.nullable(ChatCompletionStreamOptions$inboundSchema)
    .optional(),
  temperature: z.nullable(z.number().default(1)),
  top_p: z.nullable(z.number().default(1)),
  tools: z.array(ChatCompletionTool$inboundSchema).optional(),
  tool_choice: ChatCompletionToolChoiceOption$inboundSchema.optional(),
  parallel_tool_calls: z.boolean().default(false),
  user: z.string().optional(),
  function_call: z.union([
    ChatCompletionFunctionCallOption$inboundSchema,
    One$inboundSchema,
  ]).optional(),
  functions: z.array(ChatCompletionFunctions$inboundSchema).optional(),
}).transform((v) => {
  return remap$(v, {
    "frequency_penalty": "frequencyPenalty",
    "logit_bias": "logitBias",
    "top_logprobs": "topLogprobs",
    "max_tokens": "maxTokens",
    "max_completion_tokens": "maxCompletionTokens",
    "presence_penalty": "presencePenalty",
    "response_format": "responseFormat",
    "service_tier": "serviceTier",
    "stream_options": "streamOptions",
    "top_p": "topP",
    "tool_choice": "toolChoice",
    "parallel_tool_calls": "parallelToolCalls",
    "function_call": "functionCall",
  });
});

/** @internal */
export type CreateChatCompletionRequest$Outbound = {
  messages: Array<ChatCompletionRequestMessage$Outbound>;
  model: string | string;
  frequency_penalty: number | null;
  logit_bias?: { [k: string]: number } | null | undefined;
  logprobs: boolean | null;
  top_logprobs?: number | null | undefined;
  max_tokens?: number | null | undefined;
  max_completion_tokens?: number | null | undefined;
  n: number | null;
  presence_penalty: number | null;
  response_format?:
    | ResponseFormatText$Outbound
    | ResponseFormatJsonObject$Outbound
    | ResponseFormatJsonSchema$Outbound
    | undefined;
  seed?: number | null | undefined;
  service_tier: string | null;
  stop?: string | Array<string> | null | undefined;
  stream: boolean | null;
  stream_options?: ChatCompletionStreamOptions$Outbound | null | undefined;
  temperature: number | null;
  top_p: number | null;
  tools?: Array<ChatCompletionTool$Outbound> | undefined;
  tool_choice?: ChatCompletionToolChoiceOption$Outbound | undefined;
  parallel_tool_calls: boolean;
  user?: string | undefined;
  function_call?:
    | ChatCompletionFunctionCallOption$Outbound
    | string
    | undefined;
  functions?: Array<ChatCompletionFunctions$Outbound> | undefined;
};

/** @internal */
export const CreateChatCompletionRequest$outboundSchema: z.ZodType<
  CreateChatCompletionRequest$Outbound,
  z.ZodTypeDef,
  CreateChatCompletionRequest
> = z.object({
  messages: z.array(ChatCompletionRequestMessage$outboundSchema),
  model: z.union([z.string(), Two$outboundSchema]),
  frequencyPenalty: z.nullable(z.number().default(0)),
  logitBias: z.nullable(z.record(z.number().int())).optional(),
  logprobs: z.nullable(z.boolean().default(false)),
  topLogprobs: z.nullable(z.number().int()).optional(),
  maxTokens: z.nullable(z.number().int()).optional(),
  maxCompletionTokens: z.nullable(z.number().int()).optional(),
  n: z.nullable(z.number().int().default(1)),
  presencePenalty: z.nullable(z.number().default(0)),
  responseFormat: z.union([
    ResponseFormatText$outboundSchema,
    ResponseFormatJsonObject$outboundSchema,
    ResponseFormatJsonSchema$outboundSchema,
  ]).optional(),
  seed: z.nullable(z.number().int()).optional(),
  serviceTier: z.nullable(ServiceTier$outboundSchema).default(null),
  stop: z.nullable(z.union([z.string(), z.array(z.string())])).optional(),
  stream: z.nullable(z.boolean().default(false)),
  streamOptions: z.nullable(ChatCompletionStreamOptions$outboundSchema)
    .optional(),
  temperature: z.nullable(z.number().default(1)),
  topP: z.nullable(z.number().default(1)),
  tools: z.array(ChatCompletionTool$outboundSchema).optional(),
  toolChoice: ChatCompletionToolChoiceOption$outboundSchema.optional(),
  parallelToolCalls: z.boolean().default(true),
  user: z.string().optional(),
  functionCall: z.union([
    ChatCompletionFunctionCallOption$outboundSchema,
    One$outboundSchema,
  ]).optional(),
  functions: z.array(ChatCompletionFunctions$outboundSchema).optional(),
}).transform((v) => {
  return remap$(v, {
    frequencyPenalty: "frequency_penalty",
    logitBias: "logit_bias",
    topLogprobs: "top_logprobs",
    maxTokens: "max_tokens",
    maxCompletionTokens: "max_completion_tokens",
    presencePenalty: "presence_penalty",
    responseFormat: "response_format",
    serviceTier: "service_tier",
    streamOptions: "stream_options",
    topP: "top_p",
    toolChoice: "tool_choice",
    parallelToolCalls: "parallel_tool_calls",
    functionCall: "function_call",
  });
});

/**
 * @internal
 * @deprecated This namespace will be removed in future versions. Use schemas and types that are exported directly from this module.
 */
export namespace CreateChatCompletionRequest$ {
  /** @deprecated use `CreateChatCompletionRequest$inboundSchema` instead. */
  export const inboundSchema = CreateChatCompletionRequest$inboundSchema;
  /** @deprecated use `CreateChatCompletionRequest$outboundSchema` instead. */
  export const outboundSchema = CreateChatCompletionRequest$outboundSchema;
  /** @deprecated use `CreateChatCompletionRequest$Outbound` instead. */
  export type Outbound = CreateChatCompletionRequest$Outbound;
}
